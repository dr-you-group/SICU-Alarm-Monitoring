#!/usr/bin/env python3
"""
PKL íŒŒì¼ë“¤ì„ SQLite ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
ê° í™˜ìë³„ í…Œì´ë¸” ìƒì„± (í…Œì´ë¸”ëª… = AlsUnitNo)
"""

import pickle
import sqlite3
import pandas as pd
import numpy as np
import json
from pathlib import Path
from datetime import datetime
import sys
import traceback

def json_encoder(obj):
    """JSON encoderë¡œ ì²˜ë¦¬í•  ìˆ˜ ì—†ëŠ” ê°ì²´ë“¤ ì²˜ë¦¬"""
    if isinstance(obj, (pd.Timestamp, datetime)):
        return str(obj)
    elif isinstance(obj, (np.integer, np.floating)):
        return obj.item()
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    # pd.isnaëŠ” ë‹¨ì¼ ê°’ì— ëŒ€í•´ì„œë§Œ ì•ˆì „í•˜ê²Œ ì²´í¬
    else:
        try:
            if pd.isna(obj):
                return None
        except:
            pass
        return str(obj)  # ìµœí›„ì˜ ìˆ˜ë‹¨: ë¬¸ìì—´ë¡œ ë³€í™˜

def serialize_value(value):
    """ë³µì¡í•œ ë°ì´í„° íƒ€ì…ì„ JSON ë¬¸ìì—´ë¡œ ì§ë ¬í™”"""
    # None ì²´í¬
    if value is None:
        return None
    
    # ë¦¬ìŠ¤íŠ¸ë‚˜ numpy ë°°ì—´ ì²´í¬
    if isinstance(value, (list, np.ndarray)):
        try:
            # json.dumpsì— default encoder ì‚¬ìš© - ëª¨ë“  íƒ€ì… ì²˜ë¦¬
            if isinstance(value, np.ndarray) and value.size > 100000:
                print(f"    Warning: Large array with {value.size} elements")
                # í° ë°°ì—´ì€ ì²˜ìŒ 100000ê°œë§Œ
                truncated = value.flatten()[:100000]
                return json.dumps(truncated, default=json_encoder)
            else:
                return json.dumps(value, default=json_encoder)
        except Exception as e:
            print(f"    Warning: Failed to serialize array/list: {e}")
            # ìµœí›„ì˜ ì‹œë„: ëª¨ë“  ìš”ì†Œë¥¼ ë¬¸ìì—´ë¡œ
            try:
                if isinstance(value, np.ndarray):
                    converted = [str(item) for item in value.flatten()[:10000]]
                else:
                    converted = [str(item) for item in value[:10000]]
                return json.dumps(converted)
            except:
                return json.dumps([])
    
    # Timestamp ì²˜ë¦¬
    elif isinstance(value, (pd.Timestamp, datetime)):
        return str(value)
    
    # Boolean ì²˜ë¦¬
    elif isinstance(value, (bool, np.bool_)):
        return int(value)
    
    # Numpy ìˆ«ì íƒ€ì…
    elif isinstance(value, (np.integer, np.floating)):
        return value.item()
    
    # pandas NA - try-exceptë¡œ ì•ˆì „í•˜ê²Œ
    else:
        try:
            if pd.isna(value):
                return None
        except:
            pass
        return value

def get_sql_type(dtype, column_name):
    """pandas dtypeì„ SQLite íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
    # Boolean ì»¬ëŸ¼ë“¤
    if column_name in ['Classification', 'isView', 'isSelected']:
        return 'INTEGER'
    
    # Waveformì´ë‚˜ ë¦¬ìŠ¤íŠ¸ ë°ì´í„°
    if 'WAVEFORM' in column_name or 'Records' in column_name or column_name == 'Label':
        return 'TEXT'
    
    # dtype ê¸°ë°˜ íŒë‹¨
    dtype_str = str(dtype).lower()
    
    if 'datetime' in dtype_str or 'timestamp' in dtype_str:
        return 'TEXT'
    elif 'int' in dtype_str:
        return 'INTEGER'
    elif 'float' in dtype_str or 'double' in dtype_str:
        return 'REAL'
    elif 'bool' in dtype_str:
        return 'INTEGER'
    elif 'object' in dtype_str:
        return 'TEXT'
    else:
        return 'TEXT'

def create_table_dynamic(conn, patient_id, df):
    """DataFrameì˜ ëª¨ë“  ì»¬ëŸ¼ì„ ê¸°ë°˜ìœ¼ë¡œ ë™ì ìœ¼ë¡œ í…Œì´ë¸” ìƒì„±"""
    # í…Œì´ë¸”ëª…ì€ í™˜ì ID ê·¸ëŒ€ë¡œ ì‚¬ìš© (ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° ë°±í‹±ìœ¼ë¡œ ê°ì‹¸ê¸°)
    table_name = f"`{patient_id}`"
    
    # í…Œì´ë¸” ì»¬ëŸ¼ ì •ì˜ ìƒì„±
    columns = []
    
    for col in df.columns:
        sql_type = get_sql_type(df[col].dtype, col)
        
        # TimeStampëŠ” UNIQUE ì œì•½ ì¶”ê°€
        if col == 'TimeStamp':
            columns.append(f"{col} {sql_type} UNIQUE")
        else:
            columns.append(f"{col} {sql_type}")
    
    # CREATE TABLE ì¿¼ë¦¬
    create_query = f"""
    CREATE TABLE IF NOT EXISTS {table_name} (
        {', '.join(columns)}
    );
    """
    
    try:
        conn.execute(create_query)
        
        # ì¸ë±ìŠ¤ ì¶”ê°€
        conn.execute(f"CREATE INDEX IF NOT EXISTS idx_{patient_id}_timestamp ON {table_name} (TimeStamp);")
        
        # Classificationê³¼ isViewì— ì¸ë±ìŠ¤ ì¶”ê°€ (ìˆëŠ” ê²½ìš°ë§Œ)
        if 'Classification' in df.columns:
            conn.execute(f"CREATE INDEX IF NOT EXISTS idx_{patient_id}_classification ON {table_name} (Classification);")
        if 'isView' in df.columns:
            conn.execute(f"CREATE INDEX IF NOT EXISTS idx_{patient_id}_isview ON {table_name} (isView);")
            
        print(f"  - Created table with {len(df.columns)} columns")
        
    except Exception as e:
        print(f"Error creating table: {e}")
        raise

def convert_pkl_to_sqlite(pkl_file, conn):
    """ë‹¨ì¼ PKL íŒŒì¼ì„ SQLite í…Œì´ë¸”ë¡œ ë³€í™˜"""
    patient_id = pkl_file.stem
    print(f"Converting {patient_id}.pkl...")
    
    try:
        # PKL íŒŒì¼ ë¡œë“œ
        with open(pkl_file, 'rb') as f:
            df = pickle.load(f)
        
        print(f"  - Loaded {len(df)} rows, {len(df.columns)} columns")
        
        # ì¤‘ë³µ ì œê±° (TimeStamp ê¸°ì¤€)
        if 'TimeStamp' in df.columns:
            original_len = len(df)
            df = df.drop_duplicates(subset=['TimeStamp'], keep='first')
            if len(df) < original_len:
                print(f"  - Removed {original_len - len(df)} duplicate rows")
        
        # í…Œì´ë¸” ìƒì„± (ë™ì ìœ¼ë¡œ)
        create_table_dynamic(conn, patient_id, df)
        table_name = f"`{patient_id}`"
        
        # ë°ì´í„° ì‚½ì…
        inserted = 0
        failed = 0
        
        # ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ì»¬ëŸ¼ë“¤ ë¯¸ë¦¬ í™•ì¸
        problematic_columns = set()
        for col in df.columns:
            sample = df[col].dropna().head(1)
            if len(sample) > 0:
                val = sample.iloc[0]
                if isinstance(val, (list, np.ndarray)):
                    problematic_columns.add(col)
                    print(f"  - Column '{col}' contains array/list data")
        
        for idx, row in df.iterrows():
            values = []
            
            # ëª¨ë“  ì»¬ëŸ¼ ê°’ ì²˜ë¦¬
            for col in df.columns:
                value = row[col]
                
                # Boolean ì»¬ëŸ¼ íŠ¹ë³„ ì²˜ë¦¬
                if col in ['Classification', 'isView', 'isSelected']:
                    try:
                        if pd.notna(value):
                            if isinstance(value, (bool, np.bool_)):
                                values.append(1 if value else 0)
                            elif isinstance(value, (int, float, np.integer, np.floating)):
                                values.append(1 if value else 0)
                            else:
                                values.append(None)
                        else:
                            values.append(None)
                    except:
                        values.append(None)
                # ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ì»¬ëŸ¼ë“¤ íŠ¹ë³„ ì²˜ë¦¬
                elif col in problematic_columns:
                    try:
                        if value is None or (isinstance(value, float) and np.isnan(value)):
                            values.append(None)
                        else:
                            # json encoderë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                            values.append(json.dumps(value, default=json_encoder))
                    except Exception as e:
                        print(f"      Error in problematic column {col}: {e}")
                        values.append(None)
                else:
                    # ì¼ë°˜ ê°’ ì§ë ¬í™”
                    values.append(serialize_value(value))
            
            # INSERT ì¿¼ë¦¬ ìƒì„±
            placeholders = ','.join(['?' for _ in range(len(values))])
            column_names = ','.join(df.columns)
            insert_query = f"""
            INSERT OR REPLACE INTO {table_name} ({column_names})
            VALUES ({placeholders})
            """
            
            try:
                conn.execute(insert_query, values)
                inserted += 1
                
                if inserted % 100 == 0:  # ì§„í–‰ ìƒí™© í‘œì‹œ
                    print(f"    - Progress: {inserted}/{len(df)} rows...")
                    
            except sqlite3.IntegrityError as e:
                # UNIQUE ì œì•½ ìœ„ë°˜ (ì¤‘ë³µ TimeStamp)
                if "UNIQUE constraint failed" in str(e):
                    pass  # ë¬´ì‹œ
                else:
                    failed += 1
                    if failed <= 5:  # ì²˜ìŒ 5ê°œ ì—ëŸ¬ë§Œ ì¶œë ¥
                        print(f"    Error inserting row {idx}: {e}")
                        
            except Exception as e:
                failed += 1
                if failed <= 5:  # ì²˜ìŒ 5ê°œ ì—ëŸ¬ë§Œ ì¶œë ¥
                    print(f"    Error inserting row {idx}: {e}")
        
        conn.commit()
        print(f"  - Successfully inserted {inserted} rows")
        if failed > 0:
            print(f"  - Failed to insert {failed} rows")
        
        return True
        
    except Exception as e:
        print(f"Error converting {pkl_file}: {e}")
        traceback.print_exc()
        return False

def main():
    # DATA ë””ë ‰í† ë¦¬ í™•ì¸
    data_dir = Path("DATA")
    if not data_dir.exists():
        print(f"ERROR: DATA directory not found!")
        return
    
    # SQLite ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
    db_path = "sicu_alarms.db"
    
    # ê¸°ì¡´ DBê°€ ìˆìœ¼ë©´ ë°±ì—…
    if Path(db_path).exists():
        backup_path = f"{db_path}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        print(f"Backing up existing database to {backup_path}")
        Path(db_path).rename(backup_path)
    
    print(f"Creating SQLite database: {db_path}")
    
    conn = sqlite3.connect(db_path)
    
    # foreign_keys í™œì„±í™” (í•„ìš”í•œ ê²½ìš°)
    conn.execute("PRAGMA foreign_keys = ON")
    
    # ëª¨ë“  PKL íŒŒì¼ ë³€í™˜
    pkl_files = sorted(data_dir.glob("*.pkl"))
    print(f"Found {len(pkl_files)} PKL files to convert")
    
    success_count = 0
    for i, pkl_file in enumerate(pkl_files, 1):
        print(f"\n[{i}/{len(pkl_files)}] ", end="")
        
        if convert_pkl_to_sqlite(pkl_file, conn):
            success_count += 1
    
    conn.close()
    
    print(f"\n{'='*60}")
    print(f"âœ… Conversion complete!")
    print(f"Successfully converted {success_count}/{len(pkl_files)} PKL files")
    print(f"Database saved as: {db_path}")
    
    # ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° í™•ì¸
    db_size = Path(db_path).stat().st_size / (1024*1024)
    print(f"Database size: {db_size:.2f} MB")
    
    # ì›ë³¸ PKL íŒŒì¼ í¬ê¸°ì™€ ë¹„êµ
    pkl_size = sum(f.stat().st_size for f in pkl_files) / (1024*1024)
    print(f"Original PKL files size: {pkl_size:.2f} MB")
    print(f"Compression ratio: {(db_size/pkl_size)*100:.1f}%")
    
    # ìƒì„±ëœ í…Œì´ë¸” ëª©ë¡ í™•ì¸
    print(f"\nğŸ“Š Created tables:")
    conn = sqlite3.connect(db_path)
    cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' ORDER BY name")
    tables = cursor.fetchall()
    for table in tables:
        # sqlite_sequence í…Œì´ë¸”ì€ ì œì™¸í•˜ê³  í‘œì‹œ
        if table[0] != 'sqlite_sequence':
            cursor2 = conn.execute(f"SELECT COUNT(*) FROM `{table[0]}`")
            row_count = cursor2.fetchone()[0]
            print(f"  - {table[0]}: {row_count} rows")
    conn.close()

if __name__ == "__main__":
    main()
